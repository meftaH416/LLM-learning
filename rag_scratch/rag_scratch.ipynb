{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15a5523-6060-4b26-afad-b987125bfd95",
   "metadata": {},
   "source": [
    "# Check the property of device and versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d9b43-7e19-4af7-8147-5c64faebe925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"transformers>=4.43.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01191a-1d55-4ab0-b29b-51d3b0150d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f1410-e307-48dd-a81a-aa413e4b5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956e93b-8c79-4ec5-80e2-5a8f5271ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03032049-fc57-4d73-be91-034acc649be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to empty the VRAM:\n",
    "import gc\n",
    "import torch\n",
    "# del model, tokenizer, pipe\n",
    "\n",
    "# Flush memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8905e-d7c4-4e5f-a929-6b50b66e9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6f9c5-fbca-43be-b87f-92c7b98c4ae1",
   "metadata": {},
   "source": [
    "# Document Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94818a9-7a4e-4292-b542-3ab5d86c871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Class\n",
    "from embed_from_file import EmbeddingFromFile\n",
    "# Call the class\n",
    "pdf_path = [\"articles/GettingStarted.pdf\", \"articles/human-nutrition-text.pdf\"]\n",
    "embedder = EmbeddingFromFile(pdf_path= pdf_path, \n",
    "                             embed_path=\"articles/text_chunks_and_embeddings_df.csv\",\n",
    "                            device = \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662971c8-a7ef-49f7-bdab-bb6318eacadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markeddown text using pymupdf4llm\n",
    "md_text = embedder.markeddown_text(page_chunks=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79c41f-139f-4729-8cc6-71a32e61ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary with page num, token num, content and sentences using NLP LLM model\n",
    "pages_text = embedder.page_and_text(md_text,first_page= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bf67b-ff63-4075-8d0c-3e6902e59827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating chunked\n",
    "chunked = embedder.create_chunk(pages_and_texts = pages_text, slice_size=10)\n",
    "\n",
    "# Filtered dataset excluding chunks less than certain number of tokens\n",
    "filtered = embedder.df_with_min_chunk_length(pages_and_chunks=chunked, min_token_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214cdb1-5ca1-4cce-9c07-ba8b7df52701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding and pandas dataframe and saved the embeddings\n",
    "df_with_embeddings = embedder.sentence_embeddings(filtered, model=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f61a8-a75a-40fd-af2b-b14f223cb5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the stats i.e., chunk index vs token per chunks\n",
    "embedder.plot_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c599c37-a18f-4af8-abde-ae0f21570195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.sample(chunked, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1890f-1592-4162-8ce2-6a1b8705d1c8",
   "metadata": {},
   "source": [
    "# RAG - Search and Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82d1e7-7095-4ac4-add0-cb0edf1a539b",
   "metadata": {},
   "source": [
    "### Load the saved embedded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa48da-59c0-4572-8cff-5bcd8fb0491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"articles/text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecffd5-b8bb-447a-8dd4-dfc7d689ae09",
   "metadata": {},
   "source": [
    "### Load the sentense fransformer model and create embedding from saved CSV file or alternatively, use the previously saved embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e5312-5eea-4f2a-81e4-a25961427dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a6f0d-54b6-43ea-9d57-b8dd4efc539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "from time import perf_counter as timer\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d15ffb-fb52-434c-95da-7a129b73fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e73b29-3d63-406f-9e85-bb99ced3cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f970c7f-8330-45c7-80dc-3761502e2081",
   "metadata": {},
   "source": [
    "## Check device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f04435-9edd-4ca5-8375-f9d2cfbe50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad1663-a14f-48b2-9738-05eb62903776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following to empty the VRAM:\n",
    "import gc\n",
    "import torch\n",
    "# del model, tokenizer, pipe\n",
    "\n",
    "# Flush memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d1175-66b8-42bf-a330-4394deea1e09",
   "metadata": {},
   "source": [
    "# Load huggingface Text Generation LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8586dc-891b-4b0c-bcdc-6e17ffad5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include \"original/*\" --local-dir Llama-3.2-1B-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91a76c-5837-4e76-a8e9-6aed157a4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7c2b6-e9fd-4acc-aea5-a2c8e85de50d",
   "metadata": {},
   "source": [
    "### Check the model size and memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17c392-9446-453d-8211-0c3daae6e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e4a06-2c09-43fb-9ddb-3c572f3d432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15792d-52da-497d-9e24-ffdcc843f6d9",
   "metadata": {},
   "source": [
    "# Text Generation using LLM using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffc462-0c80-491a-9834-c53ba9ce2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860547d1-633c-45ad-8798-2bc80a2bff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# query = random.choice(query_list)\n",
    "query = query_list[0]\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b66c90-1819-4314-99c7-e9f3c94dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"\n",
    "    Based on the following context items, please answer the query.\n",
    "    Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "    Don't return the thinking, only return the answer.\n",
    "    Use the following examples as reference for the ideal answer style.\n",
    "    \\nExample 1:\n",
    "    Query: What are the fat-soluble vitamins?\n",
    "    Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "    \\nNow use the following context items to answer the user query:\n",
    "    {context}\n",
    "    \\nRelevant passages: <extract relevant passages from the context here>\n",
    "    User query: Make sure your answers are as concise as possible. {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a23a2-82e3-4ba7-8500-247924d6a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e030ff-5455-4de6-bb88-f3b74aaab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = random.choice(query_list)\n",
    "query = query_list[1]\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context \n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27913f0-43de-4953-8f84-4c5280c500fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Context items:\")\n",
    "# context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216f8ca-2b96-4dc4-bbc3-ab15d5e7df77",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "### see this blog about LaaJ: https://arize.com/blog-course/llm-as-a-judge/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5aa868-f0b7-4973-a15e-005536871edb",
   "metadata": {},
   "source": [
    "How can LLM-as-a-Judge be used for RAG Applications? \n",
    "\n",
    "Contextual relevance and faithfulness are two of the most widely-used metrics for assessing the accuracy and relevance of retrieved files of documents when leveraging LLM RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7c5cc-a29f-4a41-8651-db5a36e716b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a dataframe for all the queries from query_list\n",
    "\n",
    "\n",
    "eval_df = []\n",
    "temperature = 0.7  # Define temperature if it's not dynamic\n",
    "\n",
    "for ii in range(len(query_list)):\n",
    "    query = query_list[ii]\n",
    "\n",
    "    # Answer query with context and return context \n",
    "    answer, context_items = ask(query=query, \n",
    "                                temperature=temperature,\n",
    "                                max_new_tokens=512,\n",
    "                                return_answer_only=False)\n",
    "\n",
    "    eval_df.append({\n",
    "        \"query\": query,\n",
    "        \"temperature\": temperature,\n",
    "        \"answer\": answer.replace('<|im_end|>', '').replace('**', ''),\n",
    "        \"context_items\": \"\\n\".join([\"\".join(item['sentence_chunk']) for item in context_items]) # paragraph\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3731bc1-9b0f-4db6-80d7-b38c6f4854e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(eval_df)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a2239-1036-41b8-a2c5-585af0506996",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 2\n",
    "print(eval_df['query'][ii],\"\\n\\n\" ,eval_df['answer'][ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7b0bc-976f-4777-8b4a-e1414dc1f31a",
   "metadata": {},
   "source": [
    "# Re-check the context from the original Tafsir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80c4d9-a81d-4d70-a7dc-3dd5697e8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 0\n",
    "print(eval_df['query'][ii],\"\\n\\n\" ,eval_df['answer'][ii], \"\\n\\n\" ,eval_df['context_items'][ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fe509-a98c-4af7-ba84-cdac2c27cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open PDF and load target page\n",
    "pdf_path = \"articles/TafsirIbnKathir.pdf\" # requires PDF to be downloaded\n",
    "doc = fitz.open(pdf_path)\n",
    "page = doc.load_page(4405 - 1) # number of page \n",
    "\n",
    "# Get the image of the page\n",
    "img = page.get_pixmap(dpi=300)\n",
    "\n",
    "# Optional: save the image\n",
    "#img.save(\"output_filename.png\")\n",
    "doc.close()\n",
    "\n",
    "# Convert the Pixmap to a numpy array\n",
    "img_array = np.frombuffer(img.samples_mv, \n",
    "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
    "\n",
    "# Display the image using Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(13, 10))\n",
    "plt.imshow(img_array)\n",
    "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
    "plt.axis('off') # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c41d8-52d7-4dd4-aca2-0e0d67b460f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f9ffad3-7f1e-45bf-b37d-3a2e93583171",
   "metadata": {},
   "source": [
    "# Use Vector Database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e6a4a04-1183-4666-beec-52baee040df7",
   "metadata": {},
   "source": [
    "### Advantage of Vector Database from ChatGPT (We are gonna use faiss)\n",
    "\n",
    "First, what are you doing now?\n",
    "You're doing this:\n",
    "\n",
    "You saved text chunks (like little paragraphs) and their embeddings (fancy number versions of text).\n",
    "\n",
    "You're storing these in a Pandas DataFrame and searching using PyTorch and dot product.\n",
    "\n",
    "✅ This works! But...\n",
    "\n",
    "🤔 What’s the problem?\n",
    "Imagine your toy box has 10 toys now. Easy to find your favorite.\n",
    "But soon… it’ll have 10,000 toys. Or a million! 😱\n",
    "\n",
    "Searching through all of them one by one becomes SLOW.\n",
    "\n",
    "🦄 Solution: Use FAISS — it’s like a magic toy organizer!\n",
    "FAISS (Facebook AI Similarity Search) helps you search fast, even if you have a huge collection of embeddings.\n",
    "\n",
    "Instead of checking every toy, it says:\n",
    "\n",
    "“Hey! I already organized them in a smart way! Here's the best 5!”\n",
    "\n",
    "💡 Why use FAISS (or a vector database)?\n",
    "Thing\t       Without FAISS\t          With FAISS\n",
    "Speed\t     ❌ Slow for big data\t    ✅ Very fast\n",
    "Scaling\t     ❌ Hard to manage\t        ✅ Handles millions\n",
    "Memory\t     ❌ Everything in RAM\t    ✅ Can save to disk\n",
    "Searching\t ❌ All comparisons\t        ✅ Smart indexing\n",
    "Flexibility\t ❌ Manual loading\t        ✅ Search + Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a97a91-1d0a-4047-8a5e-d3d532ec223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac3c85-7f2c-40d3-8ddc-bfc2a4a78941",
   "metadata": {},
   "source": [
    "### Create Indexing store embedding inside vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385b344-36f5-46d8-8865-7fb9f0fd9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert to numpy (FAISS only works with numpy)\n",
    "embedding_np = embeddings.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "# Get dimension of embedding\n",
    "embedding_dim = embedding_np.shape[1]\n",
    "\n",
    "# Create the index (Flat = simple, exact search)\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # Or use IndexFlatIP for dot product\n",
    "index.add(embedding_np)  # Add all the embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5707c6fc-8fde-4c93-a48e-f12ede9eecf9",
   "metadata": {},
   "source": [
    "### Search FAISS with your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc37e991-e944-4460-bef5-ad6ba563bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_faiss(query: str, \n",
    "                        index: faiss.Index,\n",
    "                        model: SentenceTransformer=embedding_model, \n",
    "                        k: int=5):\n",
    "    query_embedding = model.encode(query).astype(\"float32\").reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return distances[0], indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22041e2f-57d0-41b1-85e6-0c0a7fca66c2",
   "metadata": {},
   "source": [
    "### Print results (same as before!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d41f7-157b-4872-a658-c5894f6922fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_faiss_results(query, index, pages_and_chunks, k=5):\n",
    "    distances, indices = retrieve_with_faiss(query, index, k=k)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    for score, idx in zip(distances, indices):\n",
    "        print(f\"Distance: {score:.4f}\")\n",
    "        print_wrapped(pages_and_chunks[idx]['sentence_chunk'])\n",
    "        print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "        print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(RAG)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
